name: Code Quality

on:
  pull_request:
    branches:
      - '**'
  push:
    branches:
      - main

jobs:
  format-check:
    name: Code Formatting (Black)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Set up Python
        run: uv python install 3.13

      - name: Install dependencies
        run: uv sync

      - name: Check formatting with Black
        run: uv run black --check src/ tests/

  lint:
    name: Linting (Ruff)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Set up Python
        run: uv python install 3.13

      - name: Install dependencies
        run: uv sync

      - name: Lint with Ruff
        run: uv run ruff check src/ tests/

  type-check:
    name: Type Checking (Mypy)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Set up Python
        run: uv python install 3.13

      - name: Install dependencies
        run: uv sync

      - name: Type check with Mypy
        run: uv run mypy src/

  test-with-coverage:
    name: Tests with Coverage
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Set up Python
        run: uv python install 3.13

      - name: Install dependencies
        run: uv sync

      # Test Execution Time Monitoring
      #
      # Expected Performance (as of 2025-10-18):
      # - Local execution: ~16-17 seconds (357 tests)
      # - CI execution: ~28 seconds (357 tests)
      # - CI overhead: ~1.7x local time (typical for GitHub Actions)
      #
      # The 60-second limit provides comfortable headroom while preventing
      # regression. If tests approach this limit, investigate with:
      #   uv run pytest --durations=10
      - name: Run tests with coverage
        run: |
          START_TIME=$(date +%s)
          uv run pytest --cov --cov-report=term-missing --cov-report=xml --cov-fail-under=60
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          echo "Test execution time: ${DURATION} seconds"

          if [ $DURATION -gt 60 ]; then
            echo "❌ ERROR: Test suite took ${DURATION} seconds, exceeding the 60-second limit"
            echo ""
            echo "Current execution time: ${DURATION}s"
            echo "Maximum allowed time: 60s"
            echo "Overage: $((DURATION - 60))s"
            echo ""
            echo "The test suite should complete in under 60 seconds to maintain fast CI feedback."
            echo "Please investigate slow tests and optimize performance."
            echo ""
            echo "To identify slow tests, run locally:"
            echo "  uv run pytest --durations=10"
            exit 1
          fi

          echo "✅ Test execution time (${DURATION}s) is within the 60-second limit"
        continue-on-error: false

      - name: Upload coverage reports
        uses: codecov/codecov-action@v5
        if: always()
        with:
          files: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  quality-summary:
    name: Quality Summary
    runs-on: ubuntu-latest
    needs: [format-check, lint, type-check, test-with-coverage]
    if: always()
    steps:
      - name: Check all jobs passed
        run: |
          if [ "${{ needs.format-check.result }}" != "success" ] || \
             [ "${{ needs.lint.result }}" != "success" ] || \
             [ "${{ needs.type-check.result }}" != "success" ] || \
             [ "${{ needs.test-with-coverage.result }}" != "success" ]; then
            echo "❌ One or more quality checks failed"
            exit 1
          fi
          echo "✅ All quality checks passed"
